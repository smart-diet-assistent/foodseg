#!/usr/bin/env python3
"""
é£Ÿç‰©åˆ†å‰²æ¨ç†è„šæœ¬
å¯¹è¾“å…¥å›¾ç‰‡è¿›è¡Œé£Ÿç‰©åˆ†å‰²è¯†åˆ«ï¼Œè¾“å‡ºè¯†åˆ«ç»“æœã€é£Ÿç‰©ç§ç±»å’Œé¢ç§¯ä¿¡æ¯
"""

import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import cv2
from PIL import Image
import matplotlib.pyplot as plt
import argparse
import json
import albumentations as A
from albumentations.pytorch import ToTensorV2

from model import create_model
from utils import create_colored_mask
from config import *


# Food category name mapping (English version)
FOOD_NAMES = {
    0: "Background",
    1: "Chocolate",
    2: "Cheese/Butter", 
    3: "Milk",
    4: "Egg",
    5: "Apple",
    6: "Banana",
    7: "Strawberry",
    8: "Mango",
    9: "Orange",
    10: "Beef Steak",
    11: "Pork",
    12: "Chicken/Duck",
    13: "Sauce",
    14: "Bread",
    15: "Corn",
    16: "Pasta",
    17: "Noodles",
    18: "Rice",
    19: "Tofu",
    20: "Eggplant",
    21: "Potato",
    22: "Garlic",
    23: "Tomato",
    24: "Scallion",
    25: "Ginger",
    26: "Lettuce",
    27: "Cucumber",
    28: "Carrot",
    29: "Cabbage",
    30: "Green Bean",
    31: "King Oyster Mushroom",
    32: "Shiitake Mushroom"
}


def load_label_mapping():
    """åŠ è½½æ ‡ç­¾æ˜ å°„æ–‡ä»¶"""
    label_mapping_file = os.path.join(DATA_DIR, 'label_mapping.json')
    if os.path.exists(label_mapping_file):
        with open(label_mapping_file, 'r') as f:
            return json.load(f)
    return None


def get_reverse_label_mapping(label_mapping):
    """è·å–åå‘æ ‡ç­¾æ˜ å°„ï¼ˆä»æ˜ å°„åçš„IDåˆ°åŸå§‹IDï¼‰"""
    if label_mapping is None:
        return None
    return {v: int(k) for k, v in label_mapping.items()}


def load_trained_model(model_path):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        tuple: (model, device) æˆ– (None, None) å¦‚æœåŠ è½½å¤±è´¥
    """
    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device('mps')
        print("ä½¿ç”¨MPS (Apple Silicon)")
    else:
        device = torch.device('cpu')
        print("ä½¿ç”¨CPU")
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = create_model(NUM_CLASSES, pretrained=False)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None, None
    
    try:
        print(f"åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        if 'epoch' in checkpoint:
            print(f"æ¨¡å‹è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
        if 'metrics' in checkpoint:
            metrics = checkpoint['metrics']
            print(f"æ¨¡å‹æ€§èƒ½ - mIoU: {metrics.get('mean_iou', 'N/A'):.4f}")
            
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None, None
    
    model = model.to(device)
    model.eval()
    
    return model, device


def preprocess_image(image_path, target_size=IMAGE_SIZE):
    """
    é¢„å¤„ç†è¾“å…¥å›¾åƒ
    
    Args:
        image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
        target_size (tuple): ç›®æ ‡å°ºå¯¸ (height, width)
    
    Returns:
        tuple: (preprocessed_tensor, original_image, original_size)
    """
    # è¯»å–å›¾åƒ
    if isinstance(image_path, str):
        original_image = cv2.imread(image_path)
        if original_image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
    else:
        original_image = image_path
    
    original_size = original_image.shape[:2]  # (H, W)
    
    # æ•°æ®å¢å¼º/é¢„å¤„ç†ç®¡é“
    transform = A.Compose([
        A.Resize(height=target_size[0], width=target_size[1]),
        A.Normalize(mean=MEAN, std=STD),
        ToTensorV2()
    ])
    
    # åº”ç”¨å˜æ¢
    transformed = transform(image=original_image)
    image_tensor = transformed['image'].unsqueeze(0)  # æ·»åŠ batchç»´åº¦
    
    return image_tensor, original_image, original_size


def postprocess_prediction(prediction, original_size):
    """
    åå¤„ç†é¢„æµ‹ç»“æœåˆ°åŸå§‹å›¾åƒå°ºå¯¸
    
    Args:
        prediction (torch.Tensor): æ¨¡å‹é¢„æµ‹ç»“æœ
        original_size (tuple): åŸå§‹å›¾åƒå°ºå¯¸ (H, W)
    
    Returns:
        numpy.ndarray: è°ƒæ•´å°ºå¯¸åçš„é¢„æµ‹æ©ç 
    """
    # ç§»é™¤batchç»´åº¦å¹¶è½¬æ¢ä¸ºnumpy
    if prediction.dim() == 4:
        prediction = prediction.squeeze(0)
    if prediction.dim() == 3:
        prediction = prediction.squeeze(0)
    
    prediction = prediction.cpu().numpy()
    
    print(f"è°ƒæ•´å‰é¢„æµ‹æ©ç å½¢çŠ¶: {prediction.shape}")
    print(f"ç›®æ ‡å°ºå¯¸: {original_size}")
    
    # è°ƒæ•´åˆ°åŸå§‹å°ºå¯¸
    if prediction.shape != original_size:
        # ç¡®ä¿predictionæ˜¯2Dçš„
        if len(prediction.shape) > 2:
            prediction = prediction.squeeze()
        
        prediction = cv2.resize(
            prediction.astype(np.float32), 
            (original_size[1], original_size[0]), 
            interpolation=cv2.INTER_NEAREST
        )
    
    print(f"è°ƒæ•´åé¢„æµ‹æ©ç å½¢çŠ¶: {prediction.shape}")
    return prediction.astype(np.uint8)


def calculate_food_areas(pred_mask, pixel_size_mm2=None):
    """
    è®¡ç®—å„ä¸ªé£Ÿç‰©ç±»åˆ«çš„é¢ç§¯
    
    Args:
        pred_mask (numpy.ndarray): é¢„æµ‹æ©ç 
        pixel_size_mm2 (float): æ¯ä¸ªåƒç´ ä»£è¡¨çš„é¢ç§¯(å¹³æ–¹æ¯«ç±³)ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›åƒç´ æ•°
    
    Returns:
        dict: å„ç±»åˆ«çš„é¢ç§¯ä¿¡æ¯
    """
    unique_classes = np.unique(pred_mask)
    class_areas = {}
    
    total_pixels = pred_mask.size
    
    for class_id in unique_classes:
        if class_id == 0:  # è·³è¿‡èƒŒæ™¯
            continue
            
        class_pixels = np.sum(pred_mask == class_id)
        class_percentage = (class_pixels / total_pixels) * 100
        
        class_name = FOOD_NAMES.get(class_id, f"Unknown_Class_{class_id}")
        
        area_info = {
            'pixels': class_pixels,
            'percentage': class_percentage,
            'name': class_name
        }
        
        if pixel_size_mm2 is not None:
            area_info['area_mm2'] = class_pixels * pixel_size_mm2
            area_info['area_cm2'] = area_info['area_mm2'] / 100
        
        class_areas[class_id] = area_info
    
    return class_areas


def create_visualization(original_image, pred_mask, class_areas, save_path=None):
    """
    åˆ›å»ºå¯è§†åŒ–ç»“æœ
    
    Args:
        original_image (numpy.ndarray): åŸå§‹å›¾åƒ
        pred_mask (numpy.ndarray): é¢„æµ‹æ©ç 
        class_areas (dict): ç±»åˆ«é¢ç§¯ä¿¡æ¯
        save_path (str): ä¿å­˜è·¯å¾„ï¼Œå¯é€‰
    
    Returns:
        matplotlib.figure.Figure: å¯è§†åŒ–å›¾åƒ
    """
    print(f"ğŸ¨ åˆ›å»ºå¯è§†åŒ– - åŸå§‹å›¾åƒå½¢çŠ¶: {original_image.shape}")
    print(f"ğŸ¨ åˆ›å»ºå¯è§†åŒ– - é¢„æµ‹æ©ç å½¢çŠ¶: {pred_mask.shape}")
    
    # ç¡®ä¿é¢„æµ‹æ©ç æ˜¯2Dçš„
    if len(pred_mask.shape) > 2:
        pred_mask = pred_mask.squeeze()
        print(f"ğŸ¨ å‹ç¼©åæ©ç å½¢çŠ¶: {pred_mask.shape}")
    
    # åˆ›å»ºå½©è‰²æ©ç 
    try:
        colored_mask = create_colored_mask(pred_mask, NUM_CLASSES)
        print(f"ğŸ¨ å½©è‰²æ©ç å½¢çŠ¶: {colored_mask.shape}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºå½©è‰²æ©ç å¤±è´¥: {str(e)}")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å½©è‰²æ©ç 
        colored_mask = np.zeros((*pred_mask.shape, 3))
        for i in range(min(NUM_CLASSES, 20)):
            mask_i = (pred_mask == i)
            colored_mask[mask_i] = plt.cm.tab20(i % 20)[:3]
    
    # ç¡®ä¿æ‰€æœ‰å›¾åƒå…·æœ‰ç›¸åŒçš„å°ºå¯¸
    original_height, original_width = original_image.shape[:2]
    
    # ç¡®ä¿å½©è‰²æ©ç ä¸åŸå§‹å›¾åƒå°ºå¯¸ä¸€è‡´
    if colored_mask.shape[:2] != (original_height, original_width):
        colored_mask = cv2.resize(colored_mask, (original_width, original_height), interpolation=cv2.INTER_NEAREST)
        print(f"ğŸ¨ è°ƒæ•´åå½©è‰²æ©ç å½¢çŠ¶: {colored_mask.shape}")
    
    # ç¡®ä¿é¢„æµ‹æ©ç ä¸åŸå§‹å›¾åƒå°ºå¯¸ä¸€è‡´
    if pred_mask.shape != (original_height, original_width):
        pred_mask = cv2.resize(pred_mask.astype(np.float32), (original_width, original_height), interpolation=cv2.INTER_NEAREST).astype(np.uint8)
        print(f"ğŸ¨ è°ƒæ•´åé¢„æµ‹æ©ç å½¢çŠ¶: {pred_mask.shape}")
    
    # åˆ›å»ºå åŠ å›¾åƒ - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•
    try:
        # ç¡®ä¿ä¸¤ä¸ªå›¾åƒéƒ½æ˜¯æ­£ç¡®çš„ç±»å‹å’Œå°ºå¯¸
        original_for_overlay = original_image.astype(np.float32)
        colored_for_overlay = (colored_mask * 255).astype(np.float32)
        
        print(f"ğŸ¨ å åŠ å‰ - åŸå§‹å›¾åƒ: {original_for_overlay.shape}, å½©è‰²æ©ç : {colored_for_overlay.shape}")
        
        # ç¡®ä¿ä¸¤ä¸ªå›¾åƒå…·æœ‰ç›¸åŒçš„å½¢çŠ¶
        if original_for_overlay.shape != colored_for_overlay.shape:
            print(f"âš ï¸ å½¢çŠ¶ä¸åŒ¹é…ï¼Œå°è¯•ä¿®å¤...")
            if len(original_for_overlay.shape) == 3 and len(colored_for_overlay.shape) == 3:
                if original_for_overlay.shape[2] != colored_for_overlay.shape[2]:
                    # é€šé“æ•°ä¸åŒ¹é…
                    if colored_for_overlay.shape[2] == 1:
                        colored_for_overlay = np.repeat(colored_for_overlay, 3, axis=2)
                    elif original_for_overlay.shape[2] == 1:
                        original_for_overlay = np.repeat(original_for_overlay, 3, axis=2)
        
        # å†æ¬¡æ£€æŸ¥å½¢çŠ¶
        if original_for_overlay.shape == colored_for_overlay.shape:
            overlay = cv2.addWeighted(original_for_overlay, 0.6, colored_for_overlay, 0.4, 0)
            overlay = np.clip(overlay, 0, 255).astype(np.uint8)
            print("âœ… æˆåŠŸåˆ›å»ºå åŠ å›¾åƒ")
        else:
            print(f"âŒ å½¢çŠ¶ä»ç„¶ä¸åŒ¹é…: {original_for_overlay.shape} vs {colored_for_overlay.shape}")
            overlay = original_image.copy()
    except Exception as e:
        print(f"âŒ åˆ›å»ºå åŠ å›¾åƒå¤±è´¥: {str(e)}")
        overlay = original_image.copy()
    
    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(20, 12))
    
    # Original image
    plt.subplot(2, 3, 1)
    plt.imshow(original_image)
    plt.title('Original Image', fontsize=14, fontweight='bold')
    plt.axis('off')
    
    # Prediction mask
    plt.subplot(2, 3, 2)
    plt.imshow(pred_mask, cmap='tab20')
    plt.title('Segmentation Mask', fontsize=14, fontweight='bold')
    plt.axis('off')
    
    # Colored mask
    plt.subplot(2, 3, 3)
    plt.imshow(colored_mask)
    plt.title('Colored Mask', fontsize=14, fontweight='bold')
    plt.axis('off')
    
    # Overlay image
    plt.subplot(2, 3, 4)
    plt.imshow(overlay)
    plt.title('Overlay Result', fontsize=14, fontweight='bold')
    plt.axis('off')
    
    # Area statistics chart
    plt.subplot(2, 3, 5)
    if class_areas:
        class_names = [info['name'] for info in class_areas.values()]
        percentages = [info['percentage'] for info in class_areas.values()]
        
        # Only show categories with area > 1%
        filtered_data = [(name, pct) for name, pct in zip(class_names, percentages) if pct > 1.0]
        
        if filtered_data:
            names, pcts = zip(*filtered_data)
            colors = plt.cm.tab20(np.linspace(0, 1, len(names)))
            
            plt.pie(pcts, labels=names, autopct='%1.1f%%', colors=colors, startangle=90)
            plt.title('Food Category Distribution', fontsize=14, fontweight='bold')
        else:
            plt.text(0.5, 0.5, 'No significant food regions detected', 
                    ha='center', va='center', transform=plt.gca().transAxes)
            plt.title('Food Category Distribution', fontsize=14, fontweight='bold')
    else:
        plt.text(0.5, 0.5, 'No food detected', 
                ha='center', va='center', transform=plt.gca().transAxes)
        plt.title('Food Category Distribution', fontsize=14, fontweight='bold')
    
    # Detailed statistics
    plt.subplot(2, 3, 6)
    plt.axis('off')
    
    if class_areas:
        # Sort by area percentage
        sorted_areas = sorted(class_areas.items(), key=lambda x: x[1]['percentage'], reverse=True)
        
        info_text = "Detected Food Categories:\n" + "="*30 + "\n"
        for class_id, info in sorted_areas:
            if info['percentage'] > 0.5:  # Only show categories with area > 0.5%
                info_text += f"{info['name']}:\n"
                info_text += f"  Pixels: {info['pixels']:,}\n"
                info_text += f"  Percentage: {info['percentage']:.2f}%\n"
                if 'area_cm2' in info:
                    info_text += f"  Area: {info['area_cm2']:.2f} cmÂ²\n"
                info_text += "\n"
        
        plt.text(0.05, 0.95, info_text, transform=plt.gca().transAxes, 
                fontsize=10, verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray", alpha=0.5))
    else:
        plt.text(0.5, 0.5, 'No food categories detected', 
                ha='center', va='center', transform=plt.gca().transAxes,
                fontsize=12, fontweight='bold')
    
    plt.suptitle('Food Segmentation Recognition Results', fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
    
    return fig


def inference_single_image(model, device, image_path, output_dir=None, show_result=True):
    """
    å¯¹å•å¼ å›¾åƒè¿›è¡Œæ¨ç†
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        device: è®¾å¤‡
        image_path (str): å›¾åƒè·¯å¾„
        output_dir (str): è¾“å‡ºç›®å½•ï¼Œå¯é€‰
        show_result (bool): æ˜¯å¦æ˜¾ç¤ºç»“æœ
    
    Returns:
        dict: æ¨ç†ç»“æœ
    """
    print(f"æ­£åœ¨å¤„ç†å›¾åƒ: {image_path}")
    
    try:
        # é¢„å¤„ç†
        image_tensor, original_image, original_size = preprocess_image(image_path)
        print(f"åŸå§‹å›¾åƒå°ºå¯¸: {original_image.shape}")
        print(f"åŸå§‹å°ºå¯¸è®°å½•: {original_size}")
        image_tensor = image_tensor.to(device)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(image_tensor)
            predictions = outputs['out']
            pred_mask = torch.argmax(predictions, dim=1)
            print(f"é¢„æµ‹è¾“å‡ºå°ºå¯¸: {pred_mask.shape}")
        
        # åå¤„ç†
        pred_mask_np = postprocess_prediction(pred_mask, original_size)
        print(f"åå¤„ç†åæ©ç å°ºå¯¸: {pred_mask_np.shape}")
        
        # è®¡ç®—é¢ç§¯
        class_areas = calculate_food_areas(pred_mask_np)
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            base_name = os.path.splitext(os.path.basename(image_path))[0]
            result_path = os.path.join(output_dir, f"{base_name}_result.png")
            mask_path = os.path.join(output_dir, f"{base_name}_mask.png")
        else:
            result_path = None
            mask_path = None
        
        # åˆ›å»ºå¯è§†åŒ–
        fig = create_visualization(original_image, pred_mask_np, class_areas, result_path)
        
        # ä¿å­˜æ©ç 
        if mask_path:
            cv2.imwrite(mask_path, pred_mask_np)
            print(f"âœ… æ©ç å·²ä¿å­˜åˆ°: {mask_path}")
        
        # æ˜¾ç¤ºç»“æœ
        if show_result:
            plt.show()
        else:
            plt.close(fig)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "="*50)
        print("è¯†åˆ«ç»“æœç»Ÿè®¡")
        print("="*50)
        
        if class_areas:
            sorted_areas = sorted(class_areas.items(), key=lambda x: x[1]['percentage'], reverse=True)
            for class_id, info in sorted_areas:
                if info['percentage'] > 0.5:
                    print(f"{info['name']}: {info['percentage']:.2f}% ({info['pixels']:,} åƒç´ )")
        else:
            print("æœªæ£€æµ‹åˆ°é£Ÿç‰©ç±»åˆ«")
        
        print("="*50)
        
        return {
            'original_image': original_image,
            'pred_mask': pred_mask_np,
            'class_areas': class_areas,
            'success': True
        }
        
    except Exception as e:
        print(f"âŒ å¤„ç†å›¾åƒæ—¶å‡ºé”™: {str(e)}")
        return {'success': False, 'error': str(e)}


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é£Ÿç‰©åˆ†å‰²æ¨ç†è„šæœ¬')
    parser.add_argument('--image', '-i', type=str, required=True,
                       help='è¾“å…¥å›¾åƒè·¯å¾„')
    parser.add_argument('--model', '-m', type=str, 
                       default=os.path.join(MODEL_SAVE_DIR, 'best_model.pth'),
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', type=str, default='./inference_output',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--no-show', action='store_true',
                       help='ä¸æ˜¾ç¤ºç»“æœå›¾åƒ')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.image):
        print(f"âŒ è¾“å…¥å›¾åƒä¸å­˜åœ¨: {args.image}")
        return
    
    # åŠ è½½æ¨¡å‹
    print("="*60)
    print("é£Ÿç‰©åˆ†å‰²æ¨ç†ç³»ç»Ÿ")
    print("="*60)
    
    model, device = load_trained_model(args.model)
    if model is None:
        return
    
    # æ‰§è¡Œæ¨ç†
    result = inference_single_image(
        model, device, args.image, 
        output_dir=args.output,
        show_result=not args.no_show
    )
    
    if result['success']:
        print("âœ… æ¨ç†å®Œæˆ!")
    else:
        print(f"âŒ æ¨ç†å¤±è´¥: {result['error']}")


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) == 1:
        # æ²¡æœ‰å‚æ•°æ—¶çš„æ¼”ç¤ºæ¨¡å¼
        print("="*60)
        print("é£Ÿç‰©åˆ†å‰²æ¨ç†æ¼”ç¤º")
        print("="*60)
        print("ç”¨æ³•ç¤ºä¾‹:")
        print("python inference_demo.py --image /path/to/image.jpg")
        print("python inference_demo.py --image image.jpg --output ./results")
        print("python inference_demo.py --image image.jpg --model models/best_model.pth --no-show")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¤ºä¾‹å›¾åƒ
        if os.path.exists('image.jpg'):
            print(f"\næ£€æµ‹åˆ°ç¤ºä¾‹å›¾åƒ: image.jpg")
            response = input("æ˜¯å¦ä½¿ç”¨æ­¤å›¾åƒè¿›è¡Œæ¼”ç¤º? (y/n): ")
            if response.lower() == 'y':
                sys.argv = ['inference_demo.py', '--image', 'image.jpg']
                main()
        
    else:
        main()
